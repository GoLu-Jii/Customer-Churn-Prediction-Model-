# ğŸ§  Customer Churn Prediction (XGBoost + Streamlit)

## ğŸ“Œ Project Overview
This project predicts **customer churn** for a telecom company using machine learning.  
The goal is to identify customers who are likely to cancel their subscription based on demographic, account, and service usage patterns.

The solution includes:
- Complete data analysis and feature engineering in Jupyter notebooks  
- A fully trained **XGBoost model** wrapped in a **pipeline**  
- A **Streamlit web app** for live churn prediction  
- Clean, reproducible structure ready for deployment or retraining  

---

## ğŸ§© Project Structure

06_CUSTOMER_CHURN/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_EDA.ipynb # Exploratory Data Analysis
â”‚ â”œâ”€â”€ 02_feature_engg_.ipynb # Feature engineering and preprocessing
â”‚ â”œâ”€â”€ 03_Deploy_Pipeline.ipynb # Model training and pipeline export
â”‚ â””â”€â”€ DATA/ # Raw and processed datasets
â”‚
  â”œâ”€â”€ App_streamlit.py # Streamlit web app
â”œâ”€â”€ final_churn_xgb_pipeline.pkl # Saved ML pipeline (preprocessing + model)
â”œâ”€â”€ Requirements.txt # Dependencies list
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ .gitignore # Ignore unnecessary files

markdown
Copy code

---

## âš™ï¸ Model Details

**Algorithm:** XGBoost Classifier  
**Handling Imbalance:** SMOTE (Synthetic Minority Oversampling)  
**Pipeline Steps:**
1. **Feature Engineering** â€“ creates derived features like:
   - `avg_monthly_from_total`
   - `tenure_per_monthly`
   - `has_streaming`
   - `is_auto_payment`
   - `has_support_services`
2. **Preprocessing** â€“ encodes categorical variables, scales numeric features  
3. **SMOTE** â€“ balances target classes before training  
4. **Model** â€“ tuned XGBoost with the following hyperparameters:
   ```python
   n_estimators=200
   learning_rate=0.01
   max_depth=4
   subsample=0.8
   colsample_bytree=0.7
   gamma=0.3
ğŸ“Š Model Performance (Test Set)
Metric	Score
Accuracy	0.75
F1 (Churn)	0.62
ROC-AUC	0.84
PR-AUC	0.63

âœ… The model prioritizes recall for churners, meaning fewer at-risk customers are missed.

ğŸš€ Streamlit App
ğŸ§° Run Locally
Clone the repository

The app returns:

Churn prediction (Yes/No)

Churn probability (0â€“1)

ğŸ§° Tech Stack
Python 3.12

Pandas, NumPy, Scikit-learn

XGBoost

Imbalanced-learn (SMOTE)

Streamlit

ğŸ§¾ Requirements
All dependencies are listed in Requirements.txt.

streamlit==1.39.0
pandas==2.2.3
numpy==1.26.4
scikit-learn==1.5.2
imbalanced-learn==0.12.3
xgboost==2.1.1
joblib==1.4.2

ğŸ“‚ Model Deployment Notes
The trained pipeline (final_churn_xgb_pipeline.pkl) already includes:

Feature engineering

Preprocessing

SMOTE

Model
So no separate steps are required during inference â€” just load and predict.

The Streamlit app calls this pipeline directly for new data input.

ğŸ’¡ Next Steps
Add feature importance visualization in Streamlit

Log predictions to a database for continuous learning

Experiment with LightGBM or CatBoost

Containerize app with Docker for production

ğŸ‘¤ Author
Gaurav Joshi
ğŸ“§ Gauravj121232@gmail.com 
ğŸ’» Focus: Machine Learning, Model Deployment, End-to-End Data Systems
